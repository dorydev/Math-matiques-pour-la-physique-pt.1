\section{Algèbre}
\subsection{Polynômes}
\textbf{Définition:}
Un polynôme $P(x)$ à coefficients dans un corps $K$ est une expression de la forme :
\[
P(x) = a_n x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0
\]
où $a_i \in K$ pour tout $i$ et $a_n \neq 0$. Le degré de $P(x)$ est $n$.

\textbf{Théorème fondamental de l'algèbre:}
Tout polynôme non constant à coefficients complexes a au moins une racine complexe.

\textbf{Division de polynômes:}
Soient $A(x)$ et $B(x)$ deux polynômes avec $B(x) \neq 0$. Il existe des polynômes uniques $Q(x)$ et $R(x)$ tels que :
\[
A(x) = B(x) Q(x) + R(x)
\]
où le degré de $R(x)$ est strictement inférieur au degré de $B(x)$.

\textbf{Décomposition en éléments simples:}
Tout polynôme $P(x)$ à coefficients réels peut être décomposé en produit de polynômes de degré 1 et de polynômes quadratiques irréductibles sur $\mathbb{R}$. Sur $\mathbb{C}$, tout polynôme peut être décomposé en produit de polynômes de degré 1.

\textbf{Exemple:}
Soit $P(x) = x^3 - 6x^2 + 11x - 6$. Les racines de $P(x)$ sont $x = 1, 2, 3$. On peut écrire :
\[
P(x) = (x - 1)(x - 2)(x - 3)
\]

\textbf{Théorème de Bézout:}
Soient $A(x)$ et $B(x)$ deux polynômes non nuls. Le plus grand commun diviseur (PGCD) de $A(x)$ et $B(x)$ peut être exprimé comme une combinaison linéaire de $A(x)$ et $B(x)$ :
\[
d(x) = A(x) u(x) + B(x) v(x)
\]
où $d(x)$ est le PGCD et $u(x)$, $v(x)$ sont des polynômes.

\textbf{Théorème de la racine multiple:}
Un polynôme $P(x)$ a une racine multiple $r$ si et seulement si $r$ est aussi une racine de sa dérivée $P'(x)$.

\textbf{Théorème de la décomposition en éléments simples:}
Soit $P(x)$ un polynôme à coefficients réels. On peut écrire :
\[
P(x) = c (x - r_1)^{m_1} (x - r_2)^{m_2} \cdots (x^2 + b_1 x + c_1)^{n_1} (x^2 + b_2 x + c_2)^{n_2} \cdots
\]
où $r_i$ sont les racines réelles, $x^2 + b_i x + c_i$ sont les polynômes quadratiques irréductibles, et $c$ est une constante.

\textbf{Exemple:}
Soit $P(x) = x^4 - 5x^2 + 6$. On peut écrire :
\[
P(x) = (x^2 - 2)(x^2 - 3)
\]

\subsection{Introduction \& Espaces vectoriels}

\textbf{Définition:}
Un espace vectoriel sur un corps $K$ est un ensemble $E$ muni de deux opérations :
\begin{itemize}
    \item \textbf{Addition:} Une opération $+ : V \times V \to V$ qui associe à chaque couples de vecteurs $(u, v)$ un vecteur $u + v$.
    \item \textbf{Multiplication:} Une opération $\cdot : K \times V \to V$ qui associe à chaque scalaire $a \in K$ et chaque vecteur $v \in V$ un vecteur $a \cdot v$.
\end{itemize}
$\forall$ $u, v, w \in V$ et tout $a, b \in K$ :
\begin{itemize}
    \item \textbf{Associativité de l'addition:} $(u + v) + w = u + (v + w)$.
    \item \textbf{Élément neutre de l'addition:} Il existe un élément $0 \in V$ tel que $u + 0 = u$.
    \item \textbf{Inverse de l'addition:} Pour tout $u \in V$, il existe un élément $-u \in V$ tel que $u + (-u) = 0$.
    \item \textbf{Commutativité de l'addition:} $u + v = v + u$.
    \item \textbf{Compatibilité de la multiplication scalaire avec la multiplication dans $K$:} $a \cdot (b \cdot v) = (a \cdot b) \cdot v$.
    \item \textbf{Élément neutre de la multiplication scalaire:} $1 \cdot v = v$ pour tout $v \in V$, où $1$ est l'élément neutre de $K$.
    \item \textbf{Distributivité de la multiplication scalaire par rapport à l'addition vectorielle:} $a \cdot (u + v) = a \cdot u + a \cdot v$.
    \item \textbf{Distributivité de la multiplication scalaire par rapport à l'addition scalaire:} $(a + b) \cdot v = a \cdot v + b \cdot v$.
\end{itemize}

\textbf{Exemples:}
\begin{itemize}
    \item L'ensemble $\mathbb{R}^n$ des $n$-uplets de réels est un espace vectoriel sur $\mathbb{R}$.
    \item L'ensemble des polynômes à coefficients dans $\mathbb{R}$ est un espace vectoriel sur $\mathbb{R}$.
\end{itemize}

\textbf{Théorème:}
Tout sous-espace vectoriel d'un espace vectoriel est lui-même un espace vectoriel.

\textbf{Combinaison linéaire:}
Une combinaison linéaire de vecteurs $v_1, v_2, \ldots, v_n \in V$ est une expression de la forme $a_1 v_1 + a_2 v_2 + \cdots + a_n v_n$ où $a_1, a_2, \ldots, a_n \in K$.

\textbf{Indépendance linéaire:}
Les vecteurs $v_1, v_2, \ldots, v_n \in V$ sont linéairement indépendants si la seule combinaison linéaire qui donne le vecteur nul est la combinaison triviale : $a_1 v_1 + a_2 v_2 + \cdots + a_n v_n = 0$ implique $a_1 = a_2 = \cdots = a_n = 0$.

\textbf{Base:}
Une base d'un espace vectoriel $V$ est un ensemble de vecteurs linéairement indépendants qui génèrent $V$. Autrement dit, tout vecteur de $V$ peut être écrit de manière unique comme une combinaison linéaire des vecteurs de la base.

\textbf{Dimension:}
La dimension d'un espace vectoriel $V$ est le nombre de vecteurs dans une base de $V$. Si $V$ possède une base finie, on dit que $V$ est de dimension finie.

\subsection{Matrices \& Applications en physique}

\subsubsection{Généralités}
\textbf{Définition:}
Soit $\mathcal{A}  \in \mathcal{M}_{m,n}(\mathbb{R})$ une matrice de taille $m \times n$ à coefficients réels. 
\[
\mathcal{A}  = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}
\]

On appelle \textit{transposée} de $\mathcal{A}$ la matrice notée $\mathcal{A}^T$ de taille $n \times m$ telle que pour tout $i \in \{1, \ldots, m\}$ et $j \in \{1, \ldots, n\}$, on a $\mathcal{A}^T_{j,i} = \mathcal{A}_{i,j}$.


\textbf{Multiplication de matrices:}

Soient $\mathcal{A} \in \mathcal{M}_{m,n}(\mathbb{R})$ et $\mathcal{B} \in \mathcal{M}_{n,p}(\mathbb{R})$ deux matrices. Le produit $\mathcal{C} = \mathcal{A} \mathcal{B}$ est une matrice $\mathcal{C} \in \mathcal{M}_{m,p}(\mathbb{R})$ définie par :
\[
\mathcal{C}_{i,j} = \sum_{k=1}^{n} \mathcal{A}_{i,k} \mathcal{B}_{k,j}
\]
pour tout $i \in \{1, \ldots, m\}$ et $j \in \{1, \ldots, p\}$.

\textbf{Exemple:}

Soit les matrices suivantes :
\[
\mathcal{A} = \begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{pmatrix}, \quad
\mathcal{B} = \begin{pmatrix}
7 & 8 \\
9 & 10 \\
11 & 12
\end{pmatrix}
\]

Calculons le produit $\mathcal{A} \mathcal{B}$ :
\[
\mathcal{C} = \mathcal{A} \mathcal{B} = \begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{pmatrix}
\begin{pmatrix}
7 & 8 \\
9 & 10 \\
11 & 12
\end{pmatrix}
\]

\[
\mathcal{C} = \begin{pmatrix}
1 \cdot 7 + 2 \cdot 9 + 3 \cdot 11 & 1 \cdot 8 + 2 \cdot 10 + 3 \cdot 12 \\
4 \cdot 7 + 5 \cdot 9 + 6 \cdot 11 & 4 \cdot 8 + 5 \cdot 10 + 6 \cdot 12
\end{pmatrix}
= \begin{pmatrix}
58 & 64 \\
139 & 154
\end{pmatrix}
\]

\textbf{Propriétés de la transposée:}

Pour deux matrices $\mathcal{A}$ et $\mathcal{B}$ de tailles compatibles, on a :
\[
(\mathcal{A} + \mathcal{B})^T = \mathcal{A}^T + \mathcal{B}^T
\]
\[
(k\mathcal{A})^T = k\mathcal{A}^T \quad \text{pour tout scalaire } k
\]
\[
(\mathcal{A} \mathcal{B})^T = \mathcal{B}^T \mathcal{A}^T
\]

\textbf{Inverse d'une matrice:}

Une matrice $\mathcal{A} \in \mathcal{M}_{n,n}(\mathbb{R})$ est dite inversible s'il existe une matrice $\mathcal{B} \in \mathcal{M}_{n,n}(\mathbb{R})$ telle que :
\[
\mathcal{A} \mathcal{B} = \mathcal{B} \mathcal{A} = \mathcal{I}_n
\]
où $\mathcal{I}_n$ est la matrice identité de taille $n$. La matrice $\mathcal{B}$ est alors unique et est notée $\mathcal{A}^{-1}$.

\textbf{Propriétés de l'inverse:}

Pour deux matrices inversibles $\mathcal{A}$ et $\mathcal{B}$ de même taille, on a :
\[
(\mathcal{A} \mathcal{B})^{-1} = \mathcal{B}^{-1} \mathcal{A}^{-1}
\]
\[
(\mathcal{A}^{-1})^{-1} = \mathcal{A}
\]
\[
(\mathcal{A}^T)^{-1} = (\mathcal{A}^{-1})^T
\]

\textbf{Déterminant d'une matrice}

Le déterminant est une fonction qui associe un scalaire à une matrice carrée. Le déterminant d'une matrice \(A\) est noté \(\det(A)\) ou \(|A|\). Pour une matrice \(2 \times 2\), le déterminant est calculé comme suit :
\[
\det\begin{pmatrix}
a & b \\
c & d
\end{pmatrix} = ad - bc
\]
Pour une matrice \(3 \times 3\), le déterminant est calculé comme suit :
\[
\det\begin{pmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{pmatrix} = a(ei - fh) - b(di - fg) + c(dh - eg)
\]

De manière plus générale, on a :


\[
\det(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \det(A_{ij})
\]


\paragraph{Théorème de Cramer:}
Si \(\det(A) \neq 0\), alors le système a une solution unique donnée par :
\[
X_i = \frac{\det(A_i)}{\det(A)}
\]
où \(A_i\) est la matrice obtenue en remplaçant la \(i\)-ème colonne de \(A\) par le vecteur \(B\).

\paragraph{Théorème de Cayley-Hamilton:}
Ce théorème affirme que toute matrice carrée satisfait son propre polynôme caractéristique. Soit \(A\) une matrice carrée \(n \times n\) et \(p(\lambda) = \det(\lambda I - A)\) son polynôme caractéristique. Le théorème de Cayley-Hamilton stipule que :
\[
p(A) = 0
\]
où \(0\) est la matrice nulle de même dimension que \(A\).

\paragraph{Théorème de la matrice inverse:}
Une matrice carrée \(A\) est inversible si et seulement si \(\det(A) \neq 0\). Si \(A\) est inversible, alors son inverse est donné par :
\[
A^{-1} = \frac{1}{\det(A)} \text{adj}(A)
\]
où \(\text{adj}(A)\) est la comatrice de \(A\), c'est-à-dire la transposée de la matrice des cofacteurs de \(A\).


\subsubsection{Matrices Jacobienne et Hessienne}


\textbf{Définition:}

Soit $\mathbf{f} : \mathbb{R}^n \to \mathbb{R}^m$ une fonction vectorielle différentiable. La matrice Jacobienne de $\mathbf{f}$, notée $\mathbf{J_f}$, est la matrice $m \times n$ des dérivées partielles de $\mathbf{f}$ :
\[
\mathbf{J_f} = \begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{pmatrix}
\]

\textbf{Exemple:}

Considérons la fonction $\mathbf{f} : \mathbb{R}^2 \to \mathbb{R}^2$ définie par :
\[
\mathbf{f}(x, y) = \begin{pmatrix}
x^2 + y^2 \\
2xy
\end{pmatrix}
\]

La matrice Jacobienne de $\mathbf{f}$ est :
\[
\mathbf{J_f} = \begin{pmatrix}
\frac{\partial (x^2 + y^2)}{\partial x} & \frac{\partial (x^2 + y^2)}{\partial y} \\
\frac{\partial (2xy)}{\partial x} & \frac{\partial (2xy)}{\partial y}
\end{pmatrix}
= \begin{pmatrix}
2x & 2y \\
2y & 2x
\end{pmatrix}
\]



Soit $f : \mathbb{R}^n \to \mathbb{R}$ une fonction scalaire deux fois différentiable. La matrice Hessienne de $f$, notée $\mathbf{H_f}$, est la matrice $n \times n$ des dérivées partielles secondes de $f$ :
\[
\mathbf{H_f} = \begin{pmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{pmatrix}
\]

\textbf{Exemple:}

Considérons la fonction $f : \mathbb{R}^2 \to \mathbb{R}$ définie par :
\[
f(x, y) = x^2 + xy + y^2
\]

La matrice Hessienne de $f$ est :
\[
\mathbf{H_f} = \begin{pmatrix}
\frac{\partial^2 (x^2 + xy + y^2)}{\partial x^2} & \frac{\partial^2 (x^2 + xy + y^2)}{\partial x \partial y} \\
\frac{\partial^2 (x^2 + xy + y^2)}{\partial y \partial x} & \frac{\partial^2 (x^2 + xy + y^2)}{\partial y^2}
\end{pmatrix}
= \begin{pmatrix}
2 & 1 \\
1 & 2
\end{pmatrix}
\]

\textbf{Application en physique:}
Les matrices Jacobiennes sont utilisée en physique pour étudier des systèmes dynamiques (points de stabilité, ...), quant aux matrices Hessiennes, elles permettent d'analyser la courbure des surfaces et d'optimiser la détermination de la nature des points critiques. 


\subsection{Noyau, Image}

\textbf{Définition du noyau:}
Le noyau d'une application linéaire $F : V \to W$ entre deux espaces vectoriels $V$ et $W$ est l'ensemble des vecteurs de $V$ qui sont envoyés sur le vecteur nul de $W$. Il est noté $\ker(F)$ et défini par :
\[
\ker(F) = \{ v \in V \mid F(v) = 0 \}
\]

\textbf{Définition de l'image:}
L'image d'une application linéaire $F : V \to W$ est l'ensemble des vecteurs de $W$ qui sont des images de vecteurs de $V$. Elle est notée $\operatorname{Im}(F)$ et définie par :
\[
\operatorname{Im}(F) = \{ w \in W \mid \exists v \in V, F(v) = w \}
\]

\textbf{Théorème du rang:}
Pour une application linéaire $F : V \to W$ entre deux espaces vectoriels de dimension finie, la somme des dimensions du noyau et de l'image de $F$ est égale à la dimension de $V$. Ce théorème est connu sous le nom de théorème du rang ou théorème de la dimension :
\[
\dim(\ker(F)) + \dim(\operatorname{Im}(F)) = \dim(V)
\]

\textbf{Théorème de la dimension:}
Si $F : V \to W$ est une application linéaire entre deux espaces vectoriels de dimension finie, alors :
\[
\dim(V) = \dim(\ker(F)) + \dim(\operatorname{Im}(F))
\]

\textbf{Exemple:}
Considérons l'application linéaire $F : \mathbb{R}^3 \to \mathbb{R}^2$ définie par :
\[
F(x, y, z) = (x + y, y + z)
\]
Le noyau de $F$ est donné par :
\[
\ker(F) = \{ (x, y, z) \in \mathbb{R}^3 \mid x + y = 0 \text{ et } y + z = 0 \}
\]
L'image de $F$ est donnée par :
\[
\operatorname{Im}(F) = \{ (u, v) \in \mathbb{R}^2 \mid \exists (x, y, z) \in \mathbb{R}^3, F(x, y, z) = (u, v) \}
\]

\textbf{Propriétés:}
\begin{itemize}
    \item Le noyau d'une application linéaire est un sous-espace vectoriel de l'espace de départ.
    \item L'image d'une application linéaire est un sous-espace vectoriel de l'espace d'arrivée.
    \item Une application linéaire est injective si et seulement si son noyau est réduit au vecteur nul.
    \item Une application linéaire est surjective si et seulement si son image est égale à l'espace d'arrivée.
\end{itemize}

\subsection{Corps, Anneaux, Groupes}

\subsubsection{Groupes}

\textbf{Définition:}
Un groupe est un ensemble $G$ muni d'une opération binaire $\cdot$ (souvent appelée multiplication) qui satisfait les axiomes suivants :
\begin{itemize}
    \item \textbf{Fermeture:} Pour tout $a, b \in G$, $a \cdot b \in G$.
    \item \textbf{Associativité:} Pour tout $a, b, c \in G$, $(a \cdot b) \cdot c = a \cdot (b \cdot c)$.
    \item \textbf{Élément neutre:} Il existe un élément $e \in G$ tel que pour tout $a \in G$, $a \cdot e = e \cdot a = a$.
    \item \textbf{Inverse:} Pour tout $a \in G$, il existe un élément $b \in G$ tel que $a \cdot b = b \cdot a = e$.
\end{itemize}

\textbf{Exemples:}
\begin{itemize}
    \item $(\mathbb{Z}, +)$ est un groupe abélien (commutatif).
    \item $(\mathbb{R}^*, \cdot)$ est un groupe non abélien.
\end{itemize}

\textbf{Théorème de Lagrange:}
Si $G$ est un groupe fini et $H$ est un sous-groupe de $G$, alors l'ordre de $H$ divise l'ordre de $G$.

\subsubsection{Anneaux}

\textbf{Définition:}
Un anneau est un ensemble $R$ muni de deux opérations binaires, addition $(+)$ et multiplication $(\cdot)$, satisfaisant les axiomes suivants :
\begin{itemize}
    \item $(R, +)$ est un groupe abélien.
    \item \textbf{Associativité de la multiplication:} Pour tout $a, b, c \in R$, $(a \cdot b) \cdot c = a \cdot (b \cdot c)$.
    \item \textbf{Distributivité:} Pour tout $a, b, c \in R$, $a \cdot (b + c) = (a \cdot b) + (a \cdot c)$ et $(a + b) \cdot c = (a \cdot c) + (b \cdot c)$.
\end{itemize}

\textbf{Exemples:}
\begin{itemize}
    \item $(\mathbb{Z}, +, \cdot)$ est un anneau commutatif.
    \item $(\mathbb{M}_n(\mathbb{R}), +, \cdot)$ est un anneau non commutatif.
\end{itemize}

\textbf{Théorème:}
Dans un anneau commutatif, l'ensemble des éléments inversibles forme un groupe multiplicatif.

\subsubsection{Corps}

\textbf{Définition:}
Un corps est un anneau commutatif $(K, +, \cdot)$ dans lequel tout élément non nul possède un inverse multiplicatif.

\textbf{Exemples:}
\begin{itemize}
    \item $(\mathbb{Q}, +, \cdot)$ est un corps.
    \item $(\mathbb{R}, +, \cdot)$ est un corps.
\end{itemize}

\textbf{Théorème:}
Tout corps est un anneau intègre, c'est-à-dire qu'il ne contient pas de diviseurs de zéro.

\subsubsection{Théorie de Galois}

\textbf{Définition:}
La théorie de Galois étudie les extensions de corps et les solutions des équations polynomiales en utilisant les groupes de symétrie des racines.

\textbf{Applications:}
\begin{itemize}
    \item \textbf{Résolution des équations polynomiales:} La théorie de Galois permet de déterminer si une équation polynomiale est résoluble par radicaux.
    \item \textbf{Construction des corps finis:} Elle est utilisée pour comprendre la structure des corps finis et leurs applications en cryptographie.
    \item \textbf{Problèmes de constructibilité:} Elle aide à résoudre des problèmes classiques de géométrie, comme la trisection de l'angle et la duplication du cube.
\end{itemize}

\textbf{Théorème fondamental de la théorie de Galois:}

Il existe une correspondance bijective entre les sous-groupes du groupe de Galois d'une extension de corps et les sous-corps intermédiaires de cette extension.


\textbf{Magmas:}

Un magma est un ensemble $M$ muni d'une opération binaire $\cdot : M \times M \to M$.

\textbf{Monoïdes:}

Un monoïde est un magma $(M, \cdot)$ qui satisfait les axiomes suivants :
\begin{itemize}
    \item \textbf{Associativité:} Pour tout $a, b, c \in M$, $(a \cdot b) \cdot c = a \cdot (b \cdot c)$.
    \item \textbf{Élément neutre:} Il existe un élément $e \in M$ tel que pour tout $a \in M$, $a \cdot e = e \cdot a = a$.
\end{itemize}

\textbf{Semi-groupes:}


Un semi-groupe est un magma $(S, \cdot)$ qui satisfait l'axiome d'associativité :
\begin{itemize}
    \item \textbf{Associativité:} Pour tout $a, b, c \in S$, $(a \cdot b) \cdot c = a \cdot (b \cdot c)$.
\end{itemize}

\textbf{Anneaux intègres:}

Un anneau intègre est un anneau commutatif $(R, +, \cdot)$ sans diviseurs de zéro, c'est-à-dire que pour tout $a, b \in R$, si $a \cdot b = 0$, alors $a = 0$ ou $b = 0$.

\textbf{Corps finis:}

Un corps fini est un corps $(K, +, \cdot)$ contenant un nombre fini d'éléments.

\textbf{Exemples:}
\begin{itemize}
    \item $\mathbb{F}_p$, le corps des entiers modulo un nombre premier $p$.
    \item $\mathbb{F}_{p^n}$, une extension de degré $n$ du corps $\mathbb{F}_p$.
\end{itemize}

\textbf{Algèbres:}

Une algèbre sur un corps $K$ est un espace vectoriel $A$ sur $K$ muni d'une opération bilinéaire $\cdot : A \times A \to A$.

\textbf{Exemples:}
\begin{itemize}
    \item Les algèbres de matrices $\mathbb{M}_n(K)$.
    \item Les algèbres de polynômes $K[x]$.
\end{itemize}

\textbf{Algèbres associatives:}

Une algèbre associative est une algèbre $(A, \cdot)$ telle que pour tout $a, b, c \in A$, $(a \cdot b) \cdot c = a \cdot (b \cdot c)$.

\textbf{Algèbres commutatives:}


Une algèbre commutative est une algèbre $(A, \cdot)$ telle que pour tout $a, b \in A$, $a \cdot b = b \cdot a$.

\textbf{Algèbres de Lie:}

Une algèbre de Lie est un espace vectoriel $L$ muni d'une opération bilinéaire $[\cdot, \cdot] : L \times L \to L$ appelée crochet de Lie, satisfaisant les axiomes suivants :
\begin{itemize}
    \item \textbf{Antisymétrie:} Pour tout $x, y \in L$, $[x, y] = -[y, x]$.
    \item \textbf{Identité de Jacobi:} Pour tout $x, y, z \in L$, $[x, [y, z]] + [y, [z, x]] + [z, [x, y]] = 0$.
\end{itemize}